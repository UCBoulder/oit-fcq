# oit-fcq

## Purpose:

This repo contains the core code scripts that we use for FCQs. This does not include the NLP scripts, which have their own dedicated repository: oit-an-fcq-nlp.

They are organized in four folders in the Home > oit-fcq > code > R directory.

**account-mgmt**: Account management files
Use these scripts when there is a change between what is in CU-SIS and the FCQ platform or when setting up a new semester.

**course-audit**: Course audit files
Use these scripts to pull CIW data, clean and evaluate for FCQ purposes, and generate course audit files for public use. Run once per week (usually on Friday) and use the output to set up the next FCQ session. Run daily during the course audit windows (March, June, October).

**create-sessions**: Create session files
Run in conjunction with the course audit files to set up weekly and final FCQ administrations. CL_Imports.R is the primary file. The others are special circumstances.

**data-mgmt**: Data management files
These are for reference and historical records purposes.

**fcq-results**: Results files
Use these files to process FCQ results at the end of each semester.

## Requirements

*State data, access, and/or software needed to run this repo, and, if applicable, how to acquire these requirements.*

## How To Run:

*Describe how to:*

  - *run the production pipeline to generate predictions*
  - *view the model evaluations and predictions*
  - *train and re-create the model and evaluations from the raw data.*

To use this repo template:
- Create a repo with this oda-ds-template as a template in a web browser, naming your new project in the form: "project-name-YEAR"
- Clone that repo (locally or in Workbench). 
- If using R, create an R Project in this new repo directory. 
- Create and restore virtual environment
  - If using R: Run renv::restore(lockfile = "renv.lock"), typing 'Y' to the prompts asking if you'd like to activate and install these packages.
  - If using Python: proceed with restoring the Python virtual environment from the py.lock file. 
- Delete the this and italicized text, and fill in this Readme with your project specific info. 
- Make an awesome project!

To add additional commonly used packages to this template renv.lock file:

- Clone the template repo, make it a project, run renv::restore() as above.
- Run renv::install("package_name") to install the packages you'd like to have permanently stored in this template.
- Run renv::snapshot(type = "all") to update the renv.lock file with the newly installed packages.
- Add, commit, and push these changes to the DS-template-2023 repo. 

## Output

*Describe the output of this repo's final pipeline.*

## Notes

*Any additional notes or background information that could be useful.*

More info on virtual environments:

https://cu-oda.atlassian.net/wiki/spaces/AET/pages/1766326576/Virtual+Environments

More info on DS Team practices:

https://cu-oda.atlassian.net/wiki/spaces/DST/pages/1745715201/Data+Science+Team+Processes

## Repo Structure

*Maybe this is too much and unnecessary?*

project-name-year <â€”â€“ Top level folder name.
- ðŸ“„ .gitignore <â€”â€“ Keep data, secrets, and other files out of version control.
- ðŸ“„ README.md <â€”â€“ Documentation for your Github repo.
- ðŸ“‚ code <â€”â€“ All analytic code goes under here. 
  - ðŸ“‚ SQL <â€”â€“ Raw SQL files used to pull data.
  - ðŸ“‚ Python
    - ðŸ“„ 0_config_utils_functions.py <â€”â€“ Path variable and all custom functions used in Python directory.
    - ðŸ“‚ exploratory_sandbox <â€”â€“ Sandbox for experimentation and exploration: EDA, general analysis, model training attempts. 
      - ðŸ“„ 1_load.py <â€”â€“ Load data, libraries, packages, macros, methods.
      - ðŸ“„ 2_clean.py <â€”â€“ Cleaning to turn a raw data set into an analytic data set.
    - ðŸ“‚ model_building <â€”â€“ Pipeline that trains the final model.
      - ðŸ“„ 1_load.py <â€”â€“ Load data, libraries, packages, macros, methods.
      - ðŸ“„ 2_clean.py <â€”â€“ Cleaning to turn a raw data set into an analytic data set.
      - ðŸ“„ 3_model.py <â€”â€“ Model training pipeline.
      - ðŸ“„ 4_test.py <â€”â€“ Model test on out of sample data.
      - ðŸ“„ master_script.py <â€”â€“ Simple script that runs/sources the other scripts in this directory.
    - ðŸ“‚ prediction_pipeline <â€”â€“ Pipeline that is put into production and generates a product (change name if not predictions).
      - ðŸ“„ 1_load.py <â€”â€“ Load data, libraries, packages, macros, methods.
      - ðŸ“„ 2_clean.py <â€”â€“ Cleaning to turn a raw data set into an analytic data set.
      - ðŸ“„ 3_predict.py <â€”â€“ Model training pipeline.
      - ðŸ“„ master_script.py <â€”â€“ Simple script that runs/sources the other scripts in this directory.
  - ðŸ“‚ R
    - ðŸ“„ 0_config_utils_functions.R <â€”â€“ Path variable and all custom functions used in R directory.
    - ðŸ“‚ exploratory_sandbox <â€”â€“ Sandbox for experimentation and exploration: EDA, general analysis, model training attempts.
      - ðŸ“„ 1_load.R <â€”â€“ Load data, libraries, packages, macros, methods.
      - ðŸ“„ 2_clean.R <â€”â€“ Cleaning to turn a raw data set into an analytic data set.
      - ðŸ“„ sandbox.Rmd <â€”â€“ Explore and write code in a notebook if you prefer as an alternative to script based EDA. 
    - ðŸ“‚ model_building <â€”â€“ Pipeline that trains the final model.
      - ðŸ“„ 1_load.R <â€”â€“ Load data, libraries, packages, macros, methods.
      - ðŸ“„ 2_clean.R <â€”â€“ Cleaning to turn a raw data set into an analytic data set.
      - ðŸ“„ 3_model.R <â€”â€“ Model training pipeline.
      - ðŸ“„ 4_test.R <â€”â€“ Model test on out of sample data.
      - ðŸ“„ master_script.R <â€”â€“ Simple script that runs/sources the other scripts in this directory.
    - ðŸ“‚ prediction_pipeline <â€”â€“ Pipeline that is put into production and generates a product (change name if not predictions).
      - ðŸ“„ 1_load.R <â€”â€“ Load data, libraries, packages, macros, methods.
      - ðŸ“„ 2_clean.R <â€”â€“ Cleaning to turn a raw data set into an analytic data set.
      - ðŸ“„ 3_predict.R <â€”â€“ Model training pipeline.
      - ðŸ“„ master_script.R <â€”â€“ Simple script that runs/sources the other scripts in this directory.
- ðŸ“‚ data <â€”â€“ Top level for all data. Be sure to add to your .gitignore
  - ðŸ“‚ final <â€”â€“ Files are fully ready for ML models or distributed out products
  - ðŸ“‚ raw <â€”â€“ Raw data. Put them here, then leave them alone. Forever.
  - ðŸ“‚ transformed <â€”â€“ Saved data files after cleaning and transforms, but before ready for full deployment
- ðŸ“‚ output <â€”â€“ Top level file for the various project outputs, artifacts, presentations
  - ðŸ“‚ pres_reports <â€”â€“ Powerpoint, PDF, HTML and similar reports go here, including Rmd's that knit to ppt
  - ðŸ“‚ viz_files <â€”â€“ Saved versions of stable viz files for use in other reports, PowerPoints or emailing
    - ðŸ“„ template.pptx <â€”â€“ Blank Powerpoint file that needs to be in the same directory as an .Rmd to serve as a template for formatting.
- ðŸ“‚ saved_models <â€”â€“ Top level folder for models. Store in-progress or model attempts outside final directory.
  - ðŸ“‚ final <â€”â€“ Final model that is ready for deployment or production. If there are multiple models here, consider making another repo.
